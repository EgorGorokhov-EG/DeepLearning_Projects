# My Implementation of a Neural Nretwork

This is implementation mostly based on Andrew Ng's course about Deep Learning.
Currently this version of the NN doesn't perform properly and I'm working on it, for now it almost doesn't learn if I'm using ReLU function.

## Features for now:
- 3 activation functions: ReLU, Leaky ReLU and Sigmoid
- L2 regularization
- Dropout regularization(with tunable probability of keeping unit in a layer)
- Mini-batch Gradient Descent available
