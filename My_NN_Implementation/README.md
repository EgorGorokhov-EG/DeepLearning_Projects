# My Implementation of a Neural Nretwork

This is implementation mostly based on Andrew Ng's course about Deep Learning.
Currently this version of the NN doesn't perform properly and I'm working on it. As soon as I achieve decent result, I'll add an example of it's performance.

## Features for now:
- 3 activation functions: ReLU, Leaky ReLU and Sigmoid
- L2 regularization
- Dropout regularization(with tunable probability of keeping unit in a layer)
- Mini-batch Gradient Descent available
